#!/usr/bin/env python3
"""
æ¨æ–‡æ•°æ®åº“ä¿å­˜æ¨¡å—
- è§£æ OCR ç»“æœï¼Œæå–ç»“æ„åŒ–æ¨æ–‡æ•°æ®
- ä¿å­˜åˆ° PostgreSQL æ•°æ®åº“
"""

import re
from dataclasses import dataclass
from datetime import datetime
from typing import Optional

import psycopg2
from psycopg2.extras import execute_values


# æ•°æ®åº“è¿æ¥é…ç½® (Docker PostgreSQL)
DB_CONFIG = {
    "host": "172.21.0.3",
    "port": 5432,
    "user": "cloudreve",
    "password": "",  # trust auth
    "database": "cloudreve",
}


@dataclass
class Tweet:
    """æ¨æ–‡æ•°æ®ç»“æ„"""
    author: str  # @handle
    author_name: str  # æ˜¾ç¤ºå
    content: str
    tweet_url: Optional[str] = None
    likes: Optional[int] = None
    retweets: Optional[int] = None
    views: Optional[int] = None
    created_at: Optional[datetime] = None
    voice_file: Optional[str] = None
    voice_text: Optional[str] = None
    is_liked: bool = False
    is_bookmarked: bool = False
    liked_at: Optional[datetime] = None
    bookmarked_at: Optional[datetime] = None
    # XHR æ‰©å±•å­—æ®µ
    tweet_id: Optional[str] = None
    reply_count: Optional[int] = None
    quote_count: Optional[int] = None
    user_followers: Optional[int] = None
    user_friends: Optional[int] = None
    user_description: Optional[str] = None
    data_source: str = "ocr"  # 'ocr' or 'xhr'
    raw_json: Optional[dict] = None


def parse_count(text: str) -> Optional[int]:
    """è§£ææ•°å­—ï¼ˆæ”¯æŒ K/M åç¼€ï¼‰"""
    if not text:
        return None
    text = text.strip().upper().replace(",", "")
    try:
        if "K" in text:
            return int(float(text.replace("K", "")) * 1000)
        elif "M" in text:
            return int(float(text.replace("M", "")) * 1_000_000)
        else:
            return int(float(text))
    except (ValueError, TypeError):
        return None


def parse_ocr_to_tweets(ocr_text: str) -> list[Tweet]:
    """
    è§£æ OCR æ–‡æœ¬ï¼Œæå–æ¨æ–‡åˆ—è¡¨
    
    OCR æ ¼å¼é€šå¸¸æ˜¯ï¼š
    - ç”¨æˆ·åå’Œ @handle
    - æ—¶é—´ï¼ˆå¦‚ "3h", "May 15"ï¼‰
    - æ­£æ–‡å†…å®¹
    - äº’åŠ¨æ•°æ®ï¼ˆå›å¤ã€è½¬æ¨ã€ç‚¹èµã€æµè§ˆé‡ç­‰ï¼‰
    """
    tweets = []
    
    # ç”¨æ­£åˆ™åŒ¹é…æ¨æ–‡å—
    # å…¸å‹æ¨¡å¼: @handle Â· æ—¶é—´
    # æˆ–è€…: æ˜¾ç¤ºå\n@handle
    
    # æŒ‰è¡Œåˆ†å‰²
    lines = ocr_text.strip().split("\n")
    
    current_tweet = None
    content_lines = []
    
    # åŒ¹é… @ç”¨æˆ·å æ¨¡å¼
    handle_pattern = re.compile(r"@([A-Za-z0-9_]+)")
    # åŒ¹é…äº’åŠ¨æ•°æ®æ¨¡å¼ï¼ˆæ•°å­—+K/Mï¼‰
    stats_pattern = re.compile(r"^[\d,.]+[KMkm]?$")
    # åŒ¹é…æ—¶é—´æ¨¡å¼
    time_pattern = re.compile(r"^\d+[hms]$|^\d+å°æ—¶$|^[A-Z][a-z]{2}\s+\d+$|^\d{1,2}æœˆ\d{1,2}æ—¥$")
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        
        # è·³è¿‡ç©ºè¡Œå’Œå™ªéŸ³
        if not line or line in ["Following", "For you", "Show more", "..."]:
            i += 1
            continue
        
        # æ£€æµ‹æ–°æ¨æ–‡çš„å¼€å§‹ï¼š@handle æ¨¡å¼
        handle_match = handle_pattern.search(line)
        
        if handle_match and (
            line.startswith("@") or 
            "Â·" in line or
            (i > 0 and not lines[i-1].strip())  # ç©ºè¡Œåçš„ @
        ):
            # ä¿å­˜ä¸Šä¸€æ¡æ¨æ–‡
            if current_tweet and content_lines:
                current_tweet.content = "\n".join(content_lines).strip()
                if current_tweet.content:  # åªä¿å­˜æœ‰å†…å®¹çš„
                    tweets.append(current_tweet)
            
            # å¼€å§‹æ–°æ¨æ–‡
            author = handle_match.group(1)
            author_name = ""
            
            # å°è¯•ä»ä¸Šä¸€è¡Œè·å–æ˜¾ç¤ºå
            if i > 0:
                prev_line = lines[i-1].strip()
                if prev_line and not handle_pattern.match(prev_line) and not stats_pattern.match(prev_line):
                    author_name = prev_line
            
            current_tweet = Tweet(
                author=f"@{author}",
                author_name=author_name,
                content=""
            )
            content_lines = []
            i += 1
            continue
        
        # æ£€æµ‹äº’åŠ¨æ•°æ®è¡Œ
        if current_tweet and stats_pattern.match(line):
            # å¯èƒ½æ˜¯ likes/retweets/views
            # é€šå¸¸æŒ‰é¡ºåºå‡ºç°ï¼Œå°è¯•è§£æ
            count = parse_count(line)
            if count is not None:
                if current_tweet.views is None and count > 100:
                    current_tweet.views = count
                elif current_tweet.likes is None:
                    current_tweet.likes = count
                elif current_tweet.retweets is None:
                    current_tweet.retweets = count
            i += 1
            continue
        
        # è·³è¿‡æ—¶é—´æˆ³è¡Œ
        if time_pattern.match(line):
            i += 1
            continue
        
        # ç´¯ç§¯å†…å®¹
        if current_tweet:
            # è¿‡æ»¤æ‰ä¸€äº› UI å™ªéŸ³
            if line not in ["Reply", "Repost", "Like", "Share", "Bookmark", "Views", 
                           "å›å¤", "è½¬æ¨", "å–œæ¬¢", "åˆ†äº«", "ä¹¦ç­¾", "æµè§ˆ"]:
                content_lines.append(line)
        
        i += 1
    
    # ä¿å­˜æœ€åä¸€æ¡æ¨æ–‡
    if current_tweet and content_lines:
        current_tweet.content = "\n".join(content_lines).strip()
        if current_tweet.content:
            tweets.append(current_tweet)
    
    return tweets


def tweet_from_xhr_json(data: dict) -> Tweet:
    """
    ä» XHR æ‹¦æˆªçš„ JSON æ•°æ®æ„é€  Tweet å¯¹è±¡
    
    Args:
        data: XHR å“åº”è§£æå‡ºçš„æ¨æ–‡å­—å…¸ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
            1. åµŒå¥—æ ¼å¼: {"tweet_id": "...", "user": {...}, ...}
            2. æ‰å¹³æ ¼å¼: {"id": "...", "user_name": "...", "screen_name": "...", ...}
    
    Returns:
        Tweet å¯¹è±¡
    """
    from dateutil import parser as date_parser
    
    # å…¼å®¹ä¸¤ç§æ•°æ®æ ¼å¼
    user = data.get("user", {})
    tweet_id = data.get("tweet_id") or data.get("id")
    user_name = user.get("name", "") or data.get("user_name", "")
    screen_name = user.get("screen_name", "") or data.get("screen_name", "")
    user_description = user.get("description", "") or data.get("user_description", "")
    user_followers = user.get("followers_count") or data.get("user_followers")
    user_friends = user.get("friends_count") or data.get("user_friends")
    
    # è§£ææ—¶é—´
    created_at = None
    if data.get("created_at"):
        try:
            created_at = date_parser.parse(data["created_at"])
        except:
            pass
    
    # æ„é€ æ¨æ–‡ URL
    tweet_url = None
    if tweet_id and screen_name:
        tweet_url = f"https://x.com/{screen_name}/status/{tweet_id}"
    
    return Tweet(
        tweet_id=tweet_id,
        author=f"@{screen_name}" if screen_name else "",
        author_name=user_name,
        content=data.get("text", ""),
        tweet_url=tweet_url,
        likes=data.get("favorite_count"),
        retweets=data.get("retweet_count"),
        reply_count=data.get("reply_count"),
        quote_count=data.get("quote_count"),
        user_followers=user_followers,
        user_friends=user_friends,
        user_description=user_description,
        created_at=created_at,
        data_source="xhr",
        raw_json=data
    )


def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    return psycopg2.connect(**DB_CONFIG)


def save_tweets(tweets: list[Tweet]) -> int:
    """
    ä¿å­˜æ¨æ–‡åˆ°æ•°æ®åº“ï¼ˆæ”¯æŒ OCR å’Œ XHR æ•°æ®ï¼‰
    
    Returns:
        æˆåŠŸä¿å­˜çš„æ¨æ–‡æ•°é‡
    """
    if not tweets:
        return 0
    
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        # å‡†å¤‡æ•°æ®
        data = []
        for t in tweets:
            # å°† raw_json è½¬ä¸º JSON å­—ç¬¦ä¸²
            import json
            raw_json_str = json.dumps(t.raw_json) if t.raw_json else None
            
            data.append((
                t.author,
                t.author_name,
                t.content,
                t.tweet_url,
                t.voice_file,
                t.voice_text,
                t.likes,
                t.retweets,
                t.views,
                t.created_at,
                t.tweet_id,
                t.reply_count,
                t.quote_count,
                t.user_followers,
                t.user_friends,
                t.user_description,
                t.data_source,
                raw_json_str
            ))
        
        # åˆ†ç¦»æœ‰ tweet_id å’Œæ²¡æœ‰ tweet_id çš„æ•°æ®
        data_with_id = [d for d in data if d[10] is not None]  # d[10] = tweet_id
        data_without_id = [d for d in data if d[10] is None]
        
        # æœ‰ tweet_id çš„ä½¿ç”¨ ON CONFLICT
        if data_with_id:
            sql_with_conflict = """
                INSERT INTO tweets 
                (author, author_name, content, tweet_url, voice_file, voice_text, 
                 likes, retweets, views, created_at,
                 tweet_id, reply_count, quote_count, user_followers, user_friends, 
                 user_description, data_source, raw_json)
                VALUES %s
                ON CONFLICT (tweet_id) 
                DO UPDATE SET
                    likes = EXCLUDED.likes,
                    retweets = EXCLUDED.retweets,
                    reply_count = EXCLUDED.reply_count,
                    quote_count = EXCLUDED.quote_count,
                    views = EXCLUDED.views
            """
            execute_values(cur, sql_with_conflict, data_with_id)
        
        # æ²¡æœ‰ tweet_id çš„ç›´æ¥æ’å…¥
        if data_without_id:
            sql_normal = """
                INSERT INTO tweets 
                (author, author_name, content, tweet_url, voice_file, voice_text, 
                 likes, retweets, views, created_at,
                 tweet_id, reply_count, quote_count, user_followers, user_friends, 
                 user_description, data_source, raw_json)
                VALUES %s
            """
            execute_values(cur, sql_normal, data_without_id)
        
        conn.commit()
        return len(data)
        
    except psycopg2.Error as e:
        print(f"âŒ æ•°æ®åº“é”™è¯¯: {e}")
        if conn:
            conn.rollback()
        return 0
    finally:
        if conn:
            conn.close()


def save_ocr_result(ocr_text: str) -> tuple[int, list[Tweet]]:
    """
    è§£æ OCR ç»“æœå¹¶ä¿å­˜åˆ°æ•°æ®åº“
    
    Args:
        ocr_text: OCR è¯†åˆ«çš„æ–‡æœ¬
    
    Returns:
        (ä¿å­˜æ•°é‡, æ¨æ–‡åˆ—è¡¨)
    """
    tweets = parse_ocr_to_tweets(ocr_text)
    saved = save_tweets(tweets)
    return saved, tweets


def mark_liked(tweet_url: str = None, tweet_id: int = None, liked: bool = True) -> bool:
    """
    æ ‡è®°æ¨æ–‡ä¸ºå·²ç‚¹èµ
    
    Args:
        tweet_url: æ¨æ–‡ URLï¼ˆä¼˜å…ˆï¼‰
        tweet_id: æ•°æ®åº“ ID
        liked: True=ç‚¹èµ, False=å–æ¶ˆç‚¹èµ
    
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    if not tweet_url and not tweet_id:
        return False
    
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        if liked:
            if tweet_url:
                cur.execute(
                    "UPDATE tweets SET is_liked = TRUE, liked_at = NOW() WHERE tweet_url = %s",
                    (tweet_url,)
                )
            else:
                cur.execute(
                    "UPDATE tweets SET is_liked = TRUE, liked_at = NOW() WHERE id = %s",
                    (tweet_id,)
                )
        else:
            if tweet_url:
                cur.execute(
                    "UPDATE tweets SET is_liked = FALSE, liked_at = NULL WHERE tweet_url = %s",
                    (tweet_url,)
                )
            else:
                cur.execute(
                    "UPDATE tweets SET is_liked = FALSE, liked_at = NULL WHERE id = %s",
                    (tweet_id,)
                )
        
        conn.commit()
        return cur.rowcount > 0
        
    except psycopg2.Error as e:
        print(f"âŒ æ•°æ®åº“é”™è¯¯: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()


def mark_bookmarked(tweet_url: str = None, tweet_id: int = None, bookmarked: bool = True) -> bool:
    """
    æ ‡è®°æ¨æ–‡ä¸ºå·²æ”¶è—
    
    Args:
        tweet_url: æ¨æ–‡ URLï¼ˆä¼˜å…ˆï¼‰
        tweet_id: æ•°æ®åº“ ID
        bookmarked: True=æ”¶è—, False=å–æ¶ˆæ”¶è—
    
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    if not tweet_url and not tweet_id:
        return False
    
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        if bookmarked:
            if tweet_url:
                cur.execute(
                    "UPDATE tweets SET is_bookmarked = TRUE, bookmarked_at = NOW() WHERE tweet_url = %s",
                    (tweet_url,)
                )
            else:
                cur.execute(
                    "UPDATE tweets SET is_bookmarked = TRUE, bookmarked_at = NOW() WHERE id = %s",
                    (tweet_id,)
                )
        else:
            if tweet_url:
                cur.execute(
                    "UPDATE tweets SET is_bookmarked = FALSE, bookmarked_at = NULL WHERE tweet_url = %s",
                    (tweet_url,)
                )
            else:
                cur.execute(
                    "UPDATE tweets SET is_bookmarked = FALSE, bookmarked_at = NULL WHERE id = %s",
                    (tweet_id,)
                )
        
        conn.commit()
        return cur.rowcount > 0
        
    except psycopg2.Error as e:
        print(f"âŒ æ•°æ®åº“é”™è¯¯: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()


def save_xhr_tweets_from_json(json_file: str) -> int:
    """
    ä» XHR æ‹¦æˆªçš„ JSON æ–‡ä»¶æ‰¹é‡ä¿å­˜æ¨æ–‡
    
    Args:
        json_file: JSON æ–‡ä»¶è·¯å¾„ï¼ˆtweets_xhr_test.json ç­‰ï¼‰
    
    Returns:
        æˆåŠŸä¿å­˜çš„æ¨æ–‡æ•°é‡
    """
    import json
    
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data_list = json.load(f)
        
        if not isinstance(data_list, list):
            print(f"âŒ JSON æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›åˆ—è¡¨")
            return 0
        
        tweets = [tweet_from_xhr_json(d) for d in data_list]
        saved = save_tweets(tweets)
        print(f"âœ… ä» {json_file} ä¿å­˜äº† {saved} æ¡æ¨æ–‡")
        return saved
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        return 0
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æå¤±è´¥: {e}")
        return 0


def get_recent_tweets(limit: int = 20, data_source: str = None) -> list[dict]:
    """
    è·å–æœ€è¿‘ä¿å­˜çš„æ¨æ–‡
    
    Args:
        limit: è¿”å›æ•°é‡
        data_source: ç­›é€‰æ¥æº ('ocr', 'xhr', None=å…¨éƒ¨)
    
    Returns:
        æ¨æ–‡åˆ—è¡¨
    """
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        if data_source:
            cur.execute("""
                SELECT id, scraped_at, author, author_name, content, 
                       likes, retweets, views, reply_count, data_source, tweet_id
                FROM tweets 
                WHERE data_source = %s
                ORDER BY scraped_at DESC 
                LIMIT %s
            """, (data_source, limit))
        else:
            cur.execute("""
                SELECT id, scraped_at, author, author_name, content, 
                       likes, retweets, views, reply_count, data_source, tweet_id
                FROM tweets 
                ORDER BY scraped_at DESC 
                LIMIT %s
            """, (limit,))
        
        columns = [desc[0] for desc in cur.description]
        return [dict(zip(columns, row)) for row in cur.fetchall()]
        
    except psycopg2.Error as e:
        print(f"âŒ æ•°æ®åº“é”™è¯¯: {e}")
        return []
    finally:
        if conn:
            conn.close()


# CLI æµ‹è¯•å…¥å£
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "list":
            # åˆ—å‡ºæœ€è¿‘çš„æ¨æ–‡
            data_source = sys.argv[2] if len(sys.argv) > 2 else None
            tweets = get_recent_tweets(10, data_source)
            for t in tweets:
                source = t.get('data_source', 'ocr')
                tid = t.get('tweet_id', 'N/A')
                print(f"[{t['scraped_at']}] [{source}] {t['author']}: {t['content'][:50]}...")
                if source == 'xhr':
                    print(f"  â””â”€ ID:{tid} ğŸ’¬{t.get('reply_count', 0)}")
        
        elif sys.argv[1] == "import-xhr":
            # å¯¼å…¥ XHR JSON æ•°æ®
            if len(sys.argv) < 3:
                print("Usage: python tweet_db.py import-xhr <json_file>")
                print("Example: python tweet_db.py import-xhr tweets_xhr_test.json")
            else:
                json_file = sys.argv[2]
                saved = save_xhr_tweets_from_json(json_file)
                print(f"\nğŸ“Š æ€»è®¡ä¿å­˜: {saved} æ¡æ¨æ–‡")
        
        elif sys.argv[1] == "test":
            # æµ‹è¯•è§£æ
            test_ocr = """
Albert Wang
@test_user Â· 3h
è¿™æ˜¯ä¸€æ¡æµ‹è¯•æ¨æ–‡
1.2K
456
10K

Another User
@another Â· 5h
Another test tweet content here
789
123
5.6K
"""
            tweets = parse_ocr_to_tweets(test_ocr)
            print(f"è§£æåˆ° {len(tweets)} æ¡æ¨æ–‡:")
            for t in tweets:
                print(f"  - {t.author} ({t.author_name}): {t.content[:30]}...")
                print(f"    likes={t.likes}, retweets={t.retweets}, views={t.views}")
    else:
        print("Usage:")
        print("  python tweet_db.py list [ocr|xhr]     - åˆ—å‡ºæœ€è¿‘æ¨æ–‡")
        print("  python tweet_db.py import-xhr <file>  - å¯¼å…¥XHR JSONæ•°æ®")
        print("  python tweet_db.py test               - æµ‹è¯•OCRè§£æ")
